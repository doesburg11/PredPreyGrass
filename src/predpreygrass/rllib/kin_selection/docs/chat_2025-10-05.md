# PredPreyGrass — Chat Session Summary (Oct 5, 2025)

This document captures a comprehensive summary of our chat session about implementing kin selection, cooperation metrics, training workflows, and experiment tooling in the PredPreyGrass project. It reflects the core objectives, technical decisions, code changes, workflows, and follow-ups discussed. It is a curated summary rather than a verbatim transcript.

## Session metadata
- Date: 2025-10-05
- Repository: `PredPreyGrass` (branch: main)
- OS: Linux
- Shell: bash

## Objectives and outcomes
- Implement lineage/“Selfish Gene” reward and cooperation metrics/plots
- Add line-of-sight (LOS) and kin-aware observation features
- Introduce explicit SHARE action (prey-first) to make cooperation measurable
- Build Tune training and a resume workflow; stabilize RLlib new APIs
- Extend online metrics (including per-type) and offline evaluation/plot tooling
- Document how to run, resume, and analyze results

## Evolution of goals
1. Enable Tier‑1 lineage reward; LOS-aware kin features; detectable cooperation
2. Provide a Tune-based resume script; make per-type kin selection measurable
3. Investigate type_2 absence in TensorBoard; ensure policies created when active
4. Add rationale into README (“Why two types”) and results with assets
5. Extend online and offline metrics, plus run guides and troubleshooting

## Key technical foundations
- Frameworks: Ray RLlib PPO (new API, Torch), Ray Tune + Optuna/ASHA
- Multi-agent environment with predators, prey, grass; per-species policies
- Observations: Dict planned but Box-only fed into RLModule for stability
- LOS constraints, energy and reproduction dynamics, kin-energy channel
- SHARE action: eligibility thresholds, LOS/respect_kin toggles, cooldowns

## Training, search, and resume
- Tuner-based training scripts following the new RLlib API stack
- Automatic policy module spec derived from a sample env
- Resume via Tuner.restore; best metric summary printed at restore time
- Resource sizing guided by CPU count; action masking disabled for stability

## Online metrics (callbacks)
- Core: helping_rate, share_attempt_rate, received_share_mean, shares_per_episode
- Per-type metrics: helping_rate_<group>, share_attempt_rate_<group>, received_share_mean_<group>
- Routing: shares_to_same_type_rate, shares_to_other_type_rate
- Population fractions: fraction_type_2_prey, fraction_type_2_predator
- Robust to list/dict infos and policy-mapped agent IDs

## Offline evaluation and plotting
- Evaluate checkpoints to CSV with bootstrap CIs; include per-type counts
- Plot helping rate time-series with CIs; overlay per-type population
- Steps documented in RUN_GUIDE.md (train, resume, tensorboard, offline eval)

## Notable design choices and fixes
- Feed Box-only observation into RLModule; avoid Dict encoder issues (“dict has no permute”)
- Disable action_mask for RLlib stability; consider reintroduction later via connectors/custom encoders
- Ensure both types have non-zero initial actives to create corresponding policies and metrics
- Fix import in kin selection config to reference base config correctly

## Observations from runs
- Helping ramps up during training; two-type runs show step-like jumps from eligibility thresholds and positive feedback
- Occasional env-runner restarts due to np.stack shape mismatches; trials recover under Tune
- Predation vs starvation: prey deaths primarily via predation; hypothesis that heavy sharing may boost survival explored

## Proposed next steps (optional)
- Per-type SHARE toggle: helpers vs non-helpers A/B experiment
- Add online death-cause and lifespan metrics (births_per_episode, avg_prey_lifespan)
- Extend offline evaluator to include death causes and lifespan summaries
- Consider reintroducing action masks with stable encoders/connectors
- Add VS Code tasks for one-click runs

## Referenced code/artifacts (high level)
- Environment (latest lineage): `src/predpreygrass/rllib/v3_1/predpreygrass_rllib_env.py`
- Multi-agent networks: `src/predpreygrass/rllib/v3_1/utils/networks.py`
- Configuration (kin selection): `config_env_kin_selection.py`, `config_env_base.py`
- PPO/Tune: `tune_ppo_kin_selection.py`, `resume_tune_ppo_kin_selection.py`
- Callbacks: `utils/helping_metrics_callback.py`, `utils/combined_callbacks.py`
- Offline tools: `analysis/eval_checkpoints_helping.py`, `analysis/plot_helping_time_series.py`
- Documentation: `kin_selection/README.md`, `RUN_GUIDE.md`, `results.md`

## Rationale: Why two types (type_1 and type_2)?
- Heritable types act as a kin proxy without requiring full ancestry tracking
- Enable controlled experiments: type_1-only, two-type, helper/non-helper
- Per-type policies surface differentiated learning dynamics
- Clearer measurements for kin-selection hypotheses and cooperation routing

## Troubleshooting notes
- If ‘ray’ not found: ensure project conda environment’s Python is used
- Avoid changing observation structure mid-run; stick to Box-only for RLModule
- Action mask disabled by default; re-enable only with compatible encoders

## Results snapshots
- Type_1-only runs show consistent helping ramps
- Two-type runs exhibit thresholded transitions and new steady states post-shift
- Assets and plots linked in `assets/images/kin_selection` and `results.md`

---
If you’d like a verbatim transcript export, let me know and I’ll create a file with the full message-by-message log in a similar format (subject to availability of the raw conversation text).
