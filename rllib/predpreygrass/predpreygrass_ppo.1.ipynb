{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48228240",
   "metadata": {},
   "source": [
    "### An attempt to integrate PredPreyGrassEnv into ray rllib 2.9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53e43fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\n",
      "env_runner_v2, 277 * self._active_episodes:  {}\n",
      "env_runner_v2, 287 * self._large_batch_threshold:  40000\n",
      "rollout_ops.py, 81 * worker_set <ray.rllib.evaluation.worker_set.WorkerSet object at 0x7f65e93c0fd0>\n",
      "rollout_ops.py, 81 * worker_set.local_worker() <ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f65ebb5ba90>\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "env_runner_v2, 710 * self._multiple_episodes_in_batch:  True\n",
      "/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\n",
      "env_runner_v2, 690 * all_agents_done:  True\n",
      "env_runner_v2, 694 * env_id:  0\n",
      "env_runner_v2, 696 * terminateds[env_id]['__all__']:  True\n",
      "env_runner_v2, 697 * truncateds[env_id]['__all__']:  False\n",
      "env_runner_v2, 698 * active_envs:  set()\n",
      "env_runner_v2, 699 * to_eval:  defaultdict(<class 'list'>, {})\n",
      "env_runner_v2, 700 * outputs:  []\n",
      "env_runner_v2, 855 * NO FAULTY EPISODE\n",
      "env_runner_v2, 863 * self._get_rollout_metrics [RolloutMetrics(episode_length=54, episode_reward=1.2000000000000028, agent_rewards={('predator_0', 'policy1'): -5.399999999999997, ('predator_1', 'policy1'): -0.3999999999999968, ('predator_2', 'policy1'): 4.600000000000016, ('prey_3', 'policy2'): 3.3500000000000014, ('prey_4', 'policy2'): 0.0, ('prey_5', 'policy2'): -0.9500000000000003}, custom_metrics={}, perf_stats={}, hist_data={}, media={}, episode_faulty=False, connector_metrics={'policy1': {'agent_connectors': {'ObsPreprocessorConnector_ms': 0.006175041198730469, 'StateBufferConnector_ms': 0.005745887756347656, 'ViewRequirementAgentConnector_ms': 0.20842552185058594}, 'action_connectors': {'ObsPreprocessorConnector_ms': 0.006175041198730469, 'StateBufferConnector_ms': 0.005745887756347656, 'ViewRequirementAgentConnector_ms': 0.20842552185058594}}, 'policy2': {'agent_connectors': {'ObsPreprocessorConnector_ms': 0.0043392181396484375, 'StateBufferConnector_ms': 0.0057220458984375, 'ViewRequirementAgentConnector_ms': 0.16286373138427734}, 'action_connectors': {'ObsPreprocessorConnector_ms': 0.0043392181396484375, 'StateBufferConnector_ms': 0.0057220458984375, 'ViewRequirementAgentConnector_ms': 0.16286373138427734}}})]\n",
      "env_runner_v2, 869 * env_id:  0\n",
      "env_runner_v2, 870 * is_done:  True\n",
      "env_runner_v2, 871 * outputs:  [RolloutMetrics(episode_length=54, episode_reward=1.2000000000000028, agent_rewards={('predator_0', 'policy1'): -5.399999999999997, ('predator_1', 'policy1'): -0.3999999999999968, ('predator_2', 'policy1'): 4.600000000000016, ('prey_3', 'policy2'): 3.3500000000000014, ('prey_4', 'policy2'): 0.0, ('prey_5', 'policy2'): -0.9500000000000003}, custom_metrics={}, perf_stats={}, hist_data={}, media={}, episode_faulty=False, connector_metrics={'policy1': {'agent_connectors': {'ObsPreprocessorConnector_ms': 0.006175041198730469, 'StateBufferConnector_ms': 0.005745887756347656, 'ViewRequirementAgentConnector_ms': 0.20842552185058594}, 'action_connectors': {'ObsPreprocessorConnector_ms': 0.006175041198730469, 'StateBufferConnector_ms': 0.005745887756347656, 'ViewRequirementAgentConnector_ms': 0.20842552185058594}}, 'policy2': {'agent_connectors': {'ObsPreprocessorConnector_ms': 0.0043392181396484375, 'StateBufferConnector_ms': 0.0057220458984375, 'ViewRequirementAgentConnector_ms': 0.16286373138427734}, 'action_connectors': {'ObsPreprocessorConnector_ms': 0.0043392181396484375, 'StateBufferConnector_ms': 0.0057220458984375, 'ViewRequirementAgentConnector_ms': 0.16286373138427734}}})]\n",
      "/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\n",
      "env_runner_v2, 741 * env_id:  0\n",
      "env_runner_v2, 742 * episode length:  54\n",
      "env_runner_v2, 743 * episode:  <ray.rllib.evaluation.episode_v2.EpisodeV2 object at 0x7f65e27ae990>\n",
      "env_runner_v2, 744 * is_dones True\n",
      "env_runner_v2, 745 * batch_builder.env_steps 0\n",
      "env_runner_v2, 746 * batch_builder.agent_steps 0\n",
      "env_runner_v2, 747 * batch_builder <ray.rllib.evaluation.collectors.simple_list_collector._PolicyCollectorGroup object at 0x7f65e27bb390>\n",
      "/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py\n",
      "episode_v2.py, 279 * agent_id predator_0\n",
      "episode_v2.py, 280 * self.policy_for(agent_id) policy1\n",
      "episode_v2.py, 281 * pre_batch SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp'])\n",
      "episode_v2.py, 282 * policy PPOTorchPolicy\n",
      "episode_v2.py, 283 * pre_batches[agent_id] ('policy1', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp']))\n",
      "/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py\n",
      "episode_v2.py, 279 * agent_id predator_1\n",
      "episode_v2.py, 280 * self.policy_for(agent_id) policy1\n",
      "episode_v2.py, 281 * pre_batch SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp'])\n",
      "episode_v2.py, 282 * policy PPOTorchPolicy\n",
      "episode_v2.py, 283 * pre_batches[agent_id] ('policy1', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp']))\n",
      "/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py\n",
      "episode_v2.py, 279 * agent_id predator_2\n",
      "episode_v2.py, 280 * self.policy_for(agent_id) policy1\n",
      "episode_v2.py, 281 * pre_batch SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp'])\n",
      "episode_v2.py, 282 * policy PPOTorchPolicy\n",
      "episode_v2.py, 283 * pre_batches[agent_id] ('policy1', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp']))\n",
      "/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py\n",
      "episode_v2.py, 279 * agent_id prey_3\n",
      "episode_v2.py, 280 * self.policy_for(agent_id) policy2\n",
      "episode_v2.py, 281 * pre_batch SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp'])\n",
      "episode_v2.py, 282 * policy PPOTorchPolicy\n",
      "episode_v2.py, 283 * pre_batches[agent_id] ('policy2', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp']))\n",
      "/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py\n",
      "episode_v2.py, 279 * agent_id prey_4\n",
      "episode_v2.py, 280 * self.policy_for(agent_id) policy2\n",
      "episode_v2.py, 281 * pre_batch SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp'])\n",
      "episode_v2.py, 282 * policy PPOTorchPolicy\n",
      "episode_v2.py, 283 * pre_batches[agent_id] ('policy2', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp']))\n",
      "/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py\n",
      "episode_v2.py, 279 * agent_id prey_5\n",
      "episode_v2.py, 280 * self.policy_for(agent_id) policy2\n",
      "episode_v2.py, 281 * pre_batch SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp'])\n",
      "episode_v2.py, 282 * policy PPOTorchPolicy\n",
      "episode_v2.py, 283 * pre_batches[agent_id] ('policy2', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp']))\n",
      "/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py\n",
      "episode_v2.py, 310 * pre-batch.is_single_trajectory(): False\n",
      "episode_v2.py, 311 * np.unique(pre_batch[SampleBatch.EPS_ID]): [663680436956429180]\n",
      "episode_v2.py, 312 * SampleBatch.EPS_ID: eps_id\n",
      "episode_v2.py, 313 * pre_batches: {'predator_0': ('policy1', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp', 'values_bootstrapped', 'advantages', 'value_targets'])), 'predator_1': ('policy1', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp', 'values_bootstrapped', 'advantages', 'value_targets'])), 'predator_2': ('policy1', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp', 'values_bootstrapped', 'advantages', 'value_targets'])), 'prey_3': ('policy2', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp', 'values_bootstrapped', 'advantages', 'value_targets'])), 'prey_4': ('policy2', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp'])), 'prey_5': ('policy2', PPOTorchPolicy, SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp']))}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Batches sent to postprocessing must only contain steps from a single trajectory.', SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp']))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 92\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#print(obs)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03maction0 = algo.compute_single_action(obs[\"predator_0\"], policy_id=\"policy1\")\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03maction1 = algo.compute_single_action(obs[\"predator_1\"], policy_id=\"policy1\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03mprint(\"action5\",action5)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m algo\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:342\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    341\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m skipped \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexception_cause\u001b[39;00m(skipped)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:339\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    341\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:852\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    844\u001b[0m     (\n\u001b[1;32m    845\u001b[0m         results,\n\u001b[1;32m    846\u001b[0m         train_iter_ctx,\n\u001b[1;32m    847\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    848\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 852\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm.py, 853 * results\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm.py, 854 * train_iter_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_iter_ctx)\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3044\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3042\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   3043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m-> 3044\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3045\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3046\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:407\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m         train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    403\u001b[0m             worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers,\n\u001b[1;32m    404\u001b[0m             max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    405\u001b[0m         )\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m         train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m            \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# New Episode-returning EnvRunner API.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mnum_remote_workers() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py:83\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrollout_ops.py, 81 * worker_set\u001b[39m\u001b[38;5;124m\"\u001b[39m, worker_set)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrollout_ops.py, 81 * worker_set.local_worker()\u001b[39m\u001b[38;5;124m\"\u001b[39m, worker_set\u001b[38;5;241m.\u001b[39mlocal_worker())\n\u001b[0;32m---> 83\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m [\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m worker_set\u001b[38;5;241m.\u001b[39mforeach_worker(\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m w: w\u001b[38;5;241m.\u001b[39msample(), local_worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, healthy_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py:694\u001b[0m, in \u001b[0;36mRolloutWorker.sample\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_start\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    688\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating sample batch of size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    690\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_rollout_fragment_length\n\u001b[1;32m    691\u001b[0m         )\n\u001b[1;32m    692\u001b[0m     )\n\u001b[0;32m--> 694\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    695\u001b[0m steps_so_far \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    696\u001b[0m     batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcount_steps_by \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39magent_steps()\n\u001b[1;32m    699\u001b[0m )\n\u001b[1;32m    701\u001b[0m \u001b[38;5;66;03m# In truncate_episodes mode, never pull more than 1 batch per env.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# This avoids over-running the target batch size.\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/sampler.py:91\u001b[0m, in \u001b[0;36mSamplerInput.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;129m@override\u001b[39m(InputReader)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[0;32m---> 91\u001b[0m     batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     92\u001b[0m     batches\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extra_batches())\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/sampler.py:276\u001b[0m, in \u001b[0;36mSyncSampler.get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;129m@override\u001b[39m(SamplerInput)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env_runner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, RolloutMetrics):\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_queue\u001b[38;5;241m.\u001b[39mput(item)\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py:348\u001b[0m, in \u001b[0;36mEnvRunnerV2.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Samples and yields training episodes continuously.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03mYields:\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    Object containing state, action, reward, terminal condition,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    and other fields as dictated by `policy`.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py:374\u001b[0m, in \u001b[0;36mEnvRunnerV2.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# types: Set[EnvID], Dict[PolicyID, List[AgentConnectorDataType]],\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m#       List[Union[RolloutMetrics, SampleBatchType]]\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m active_envs, to_eval, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_observations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43munfiltered_obs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munfiltered_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mterminateds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminateds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncateds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncateds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perf_stats\u001b[38;5;241m.\u001b[39mincr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_obs_processing_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t1)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Do batched policy eval (accross vectorized envs).\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py:701\u001b[0m, in \u001b[0;36mEnvRunnerV2._process_observations\u001b[0;34m(self, unfiltered_obs, rewards, terminateds, truncateds, infos)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_runner_v2, 699 * to_eval: \u001b[39m\u001b[38;5;124m\"\u001b[39m, to_eval)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_runner_v2, 700 * outputs: \u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs)\n\u001b[0;32m--> 701\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_done_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mterminateds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__all__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtruncateds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__all__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactive_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_runner_v2, 710 * self._multiple_episodes_in_batch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiple_episodes_in_batch)\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# Try to build something.\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py:873\u001b[0m, in \u001b[0;36mEnvRunnerV2._handle_done_episode\u001b[0;34m(self, env_id, env_obs_or_exception, is_done, active_envs, to_eval, outputs)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_runner_v2, 870 * is_done: \u001b[39m\u001b[38;5;124m\"\u001b[39m, is_done)\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_runner_v2, 871 * outputs: \u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs)\n\u001b[0;32m--> 873\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_done_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# Clean up and deleted the post-processed episode now that we have collected\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# its data.\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_episode(env_id, episode_or_exception)\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py:750\u001b[0m, in \u001b[0;36mEnvRunnerV2._build_done_episode\u001b[0;34m(self, env_id, is_done, outputs)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_runner_v2, 746 * batch_builder.agent_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_builder\u001b[38;5;241m.\u001b[39magent_steps)\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_runner_v2, 747 * batch_builder\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_builder)\n\u001b[0;32m--> 750\u001b[0m \u001b[43mepisode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_builder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_builder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_done\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_dones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_done\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# If, we are not allowed to pack the next episode into the same\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;66;03m# SampleBatch (batch_mode=complete_episodes) -> Build the\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;66;03m# MultiAgentBatch from a single episode and add it to \"outputs\".\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;66;03m# Otherwise, just postprocess and continue collecting across\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;66;03m# episodes.\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_runner_v2, 761 * self._multiple_episodes_in_batch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiple_episodes_in_batch)\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py:314\u001b[0m, in \u001b[0;36mEpisodeV2.postprocess_episode\u001b[0;34m(self, batch_builder, is_done, check_dones)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_v2.py, 312 * SampleBatch.EPS_ID:\u001b[39m\u001b[38;5;124m\"\u001b[39m,SampleBatch\u001b[38;5;241m.\u001b[39mEPS_ID)\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_v2.py, 313 * pre_batches:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pre_batches)\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches sent to postprocessing must only contain steps \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom a single trajectory.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    317\u001b[0m         pre_batch,\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pre_batches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    321\u001b[0m     other_batches \u001b[38;5;241m=\u001b[39m pre_batches\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mValueError\u001b[0m: ('Batches sent to postprocessing must only contain steps from a single trajectory.', SampleBatch(54: ['obs', 'new_obs', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'vf_preds', 'action_dist_inputs', 'action_logp']))"
     ]
    }
   ],
   "source": [
    "#NUMBER III\n",
    "#define policies in dict with PolicySpec\n",
    "#add width and height parameters to the environment\n",
    "\n",
    "from environments.predpreygrass_env import PredPreyGrassEnv\n",
    "from config.config_rllib import configuration\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.utils.pre_checks.env import  check_env\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "check_env(PredPreyGrassEnv(configuration))\n",
    "\n",
    "def env_creator(configuration):\n",
    "    return PredPreyGrassEnv(configuration)  # return an env instance\n",
    "\n",
    "register_env(\"pred_prey_grass\", env_creator)\n",
    "\n",
    "policy1 = PolicySpec()\n",
    "policy2 = PolicySpec()\n",
    "\n",
    "policies = { \n",
    "    \"policy1\": policy1,\n",
    "    \"policy2\": policy2\n",
    "}\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    if agent_id.startswith(\"predator_\"):\n",
    "        return \"policy1\"\n",
    "    if agent_id.startswith(\"prey_\"):\n",
    "        return \"policy2\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected agent ID: {agent_id}\")\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"pred_prey_grass\")\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(\n",
    "        create_env_on_local_worker=True,\n",
    "        batch_mode=\"truncate_episodes\", #\"complete_episodes\", \n",
    "        num_rollout_workers=0,\n",
    "        rollout_fragment_length= \"auto\",\n",
    "    )\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\n",
    "        \"fcnet_hiddens\" : [64, 64], \n",
    "        \"_disable_preprocessor_api\": True,\n",
    "        \"conv_filters\": [[32, [8, 8], 4], [64, [4, 4], 2], [512, [1, 1], 1]] # Copilot\n",
    "        }\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn\n",
    "    )\n",
    "    #.build()\n",
    ")\n",
    "\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "env = PredPreyGrassEnv(configuration=configuration)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "#print(obs)\n",
    "\"\"\"\n",
    "action0 = algo.compute_single_action(obs[\"predator_0\"], policy_id=\"policy1\")\n",
    "action1 = algo.compute_single_action(obs[\"predator_1\"], policy_id=\"policy1\")\n",
    "action2 = algo.compute_single_action(obs[\"predator_2\"], policy_id=\"policy1\")\n",
    "action3 = algo.compute_single_action(obs[\"prey_3\"], policy_id=\"policy2\")\n",
    "action4 = algo.compute_single_action(obs[\"prey_4\"], policy_id=\"policy2\")\n",
    "action5 = algo.compute_single_action(obs[\"prey_5\"], policy_id=\"policy2\")\n",
    "print(\"action0\",action0)\n",
    "print(\"action1\",action1)\n",
    "print(\"action2\",action2)\n",
    "print(\"action3\",action3)\n",
    "print(\"action4\",action4)\n",
    "print(\"action5\",action5)\n",
    "\"\"\"\n",
    "algo.train()\n",
    "\n",
    "algo.stop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
