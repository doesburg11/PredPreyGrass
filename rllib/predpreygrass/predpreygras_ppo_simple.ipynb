{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48228240",
   "metadata": {},
   "source": [
    "An alternative and simplified PredPreyGrass environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28d6948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________\n",
      "|.2..1 |\n",
      "|....  |\n",
      "|....  |\n",
      "|...   |\n",
      "|      |\n",
      "|      |\n",
      "‾‾‾‾‾‾‾‾\n",
      "\n",
      "R1= 9.0\n",
      "R2=-2.7 (0 collisions)\n",
      "Env timesteps=27/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     obs, rewards, terminateds, truncateds, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent1\u001b[39m\u001b[38;5;124m\"\u001b[39m: action1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent2\u001b[39m\u001b[38;5;124m\"\u001b[39m: action2})\n\u001b[1;32m     38\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m algo\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from environments.predpreygrass_simple_env import MultiAgentArena\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.utils.pre_checks.env import check_env\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.logger import pretty_print\n",
    "import ray\n",
    "from ray import train, tune\n",
    "\n",
    "import time\n",
    "#check_env(MultiAgentArena) # gives error\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
    "    .environment(env=MultiAgentArena)\n",
    "    .multi_agent(\n",
    "        policies=[\"policy1\", \"policy2\"],\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs, _ = env.reset()\n",
    "truncateds = {\"__all__\" : False}\n",
    "\n",
    "while not truncateds[\"__all__\"]:\n",
    "    action1 = algo.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = algo.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, terminateds, truncateds, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    \n",
    "algo.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b1a701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________\n",
      "|1.... |\n",
      "|....  |\n",
      "|.2..  |\n",
      "|...   |\n",
      "|      |\n",
      "|      |\n",
      "‾‾‾‾‾‾‾‾\n",
      "\n",
      "R1=-3.0\n",
      "R2=-3.9 (1 collisions)\n",
      "Env timesteps=50/50\n"
     ]
    }
   ],
   "source": [
    "def env_creator(env_config):\n",
    "    return MultiAgentArena(config=env_config)  # return an env instance\n",
    "\n",
    "register_env(\"multi_agent_arena\", env_creator)\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
    "    .environment(env=\"multi_agent_arena\",)\n",
    "    .multi_agent(\n",
    "        policies=[\"policy1\", \"policy2\"],\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs, _ = env.reset()\n",
    "truncateds = {\"__all__\" : False}\n",
    "    \n",
    "while not truncateds[\"__all__\"]:\n",
    "\n",
    "    action1 = algo.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = algo.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, terminateds, truncateds, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "algo.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a0427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define policies in dict with PolicySpec\n",
    "def env_creator(env_config):\n",
    "    return MultiAgentArena(config=env_config)  # return an env instance\n",
    "\n",
    "register_env(\"multi_agent_arena\", env_creator)\n",
    "\n",
    "policies = { \"policy1\": PolicySpec(), \"policy2\": PolicySpec() }\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"multi_agent_arena\")\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs, _ = env.reset()\n",
    "truncateds = {\"__all__\" : False}\n",
    "    \n",
    "while not truncateds[\"__all__\"]:\n",
    "\n",
    "    action1 = algo.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = algo.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, terminateds, truncateds, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "algo.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define policies in dict with PolicySpec\n",
    "#add width and height parameters to the environment\n",
    "\n",
    "MAX_WIDTH = 100\n",
    "MAX_HEIGHT = 100\n",
    "                                                   \n",
    "def env_creator(env_config):\n",
    "    print(env_config)\n",
    "    return MultiAgentArena(config=env_config, width=MAX_WIDTH, height=MAX_HEIGHT)  # return an env instance\n",
    "register_env(\"multi_agent_arena\", env_creator)\n",
    "\n",
    "policies = { \"policy1\": PolicySpec(), \"policy2\": PolicySpec() }\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    if agent_id == \"agent1\":\n",
    "        return \"policy1\"\n",
    "    else:\n",
    "        return \"policy2\"\n",
    "\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"multi_agent_arena\")\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs, _ = env.reset()\n",
    "truncateds = {\"__all__\" : False}\n",
    "    \n",
    "while not truncateds[\"__all__\"]:\n",
    "\n",
    "    action1 = algo.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = algo.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, terminateds, truncateds, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "algo.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define policies in dict with PolicySpec\n",
    "#add width and height parameters to the environment\n",
    "#train the environment with the algo\n",
    "\n",
    "MAX_WIDTH = 10\n",
    "MAX_HEIGHT = 10\n",
    "                                                   \n",
    "def env_creator(env_config):\n",
    "    print(env_config)\n",
    "    return MultiAgentArena(config=env_config, width=MAX_WIDTH, height=MAX_HEIGHT)  # return an env instance\n",
    "register_env(\"multi_agent_arena\", env_creator)\n",
    "\n",
    "policies = { \"policy1\": PolicySpec(), \"policy2\": PolicySpec() }\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    if agent_id == \"agent1\":\n",
    "        return \"policy1\"\n",
    "    else:\n",
    "        return \"policy2\"\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"multi_agent_arena\")\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "\n",
    "\n",
    "config_env = {\"render\": True}\n",
    "\n",
    "\n",
    "env = MultiAgentArena(config={\"render\": True}, width=10, height=10)\n",
    "obs, _ = env.reset()\n",
    "truncateds = {\"__all__\" : False}\n",
    "    \n",
    "while not truncateds[\"__all__\"]:\n",
    "\n",
    "    action1 = algo.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = algo.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, terminateds, truncateds, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "algo.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e06d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define policies in dict with PolicySpec\n",
    "#add width and height parameters to the environment\n",
    "#train the environment with the algo\n",
    "#use tune for a gridsearch\n",
    "\n",
    "MAX_WIDTH = 10\n",
    "MAX_HEIGHT = 10\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "                                                   \n",
    "def env_creator(env_config):\n",
    "    print(env_config)\n",
    "    return MultiAgentArena(config=env_config, width=MAX_WIDTH, height=MAX_HEIGHT)  # return an env instance\n",
    "register_env(\"multi_agent_arena\", env_creator)\n",
    "\n",
    "policies = { \"policy1\": PolicySpec(), \"policy2\": PolicySpec() }\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    if agent_id == \"agent1\":\n",
    "        return \"policy1\"\n",
    "    else:\n",
    "        return \"policy2\"\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"multi_agent_arena\")\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]},lr=tune.grid_search([0.01, 0.001, 0.0001]))\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"episode_reward_mean\": 15},\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "tuner.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define policies in dict with PolicySpec\n",
    "#add width and height parameters to the environment\n",
    "#train the environment with the algo\n",
    "#use tune for a gridsearch\n",
    "#retrieving the checkpoint(s) of the trained agent\n",
    "\n",
    "MAX_WIDTH = 10\n",
    "MAX_HEIGHT = 10\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "                                                   \n",
    "def env_creator(env_config):\n",
    "    print(env_config)\n",
    "    return MultiAgentArena(config=env_config, width=MAX_WIDTH, height=MAX_HEIGHT)  # return an env instance\n",
    "register_env(\"multi_agent_arena\", env_creator)\n",
    "\n",
    "policies = { \"policy1\": PolicySpec(), \"policy2\": PolicySpec() }\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    if agent_id == \"agent1\":\n",
    "        return \"policy1\"\n",
    "    else:\n",
    "        return \"policy2\"\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"multi_agent_arena\")\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]},lr=tune.grid_search([0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]))\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# ``Tuner.fit()`` allows setting a custom log directory (other than ``~/ray-results``)\n",
    "tuner = ray.tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"episode_reward_mean\": 10},\n",
    "        checkpoint_config=train.CheckpointConfig(checkpoint_at_end=True),\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "# Get the best result based on a particular metric.\n",
    "best_result = results.get_best_result(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# Get the best checkpoint corresponding to the best result.\n",
    "best_checkpoint = best_result.checkpoint\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
