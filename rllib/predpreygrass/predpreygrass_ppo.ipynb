{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48228240",
   "metadata": {},
   "source": [
    "### An attempt to integrate PredPreyGrassEnv into ray rllib 2.9.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d717f",
   "metadata": {},
   "source": [
    "Sourcsces of conversion problems:\n",
    "#https://discuss.ray.io/t/multiagents-type-actions-observation-space-defined-in-environement/5120/3\n",
    "#https://discuss.ray.io/t/observation-space-not-provided-in-policyspec/6501/10\n",
    "#https://colab.research.google.com/drive/19aZ_bUFJwJuPT4dRzVO2cTn8Ey0G7CoA?usp=sharing#scrollTo=7GjQY33DTz0H\n",
    "#https://github.com/ray-project/ray/blob/master/rllib/env/env_context.py\n",
    "\n",
    "The relation between the observation space and an observation is that the observation is a sample from the observation space. [chatgpt]. The question remains how to check this.\n",
    "https://stackoverflow.com/questions/76289764/understanding-action-observation-spaces-in-gym-for-custom-environments-and-age\n",
    "\n",
    "TODO: \n",
    "-observe function adjustment\n",
    "-swapaxes\n",
    "-get rid of masks and observation range for observation spaces?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6876bf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "3\n",
      "get_agent_ids {'predator_0', 'prey_4', 'predator_3', 'prey_10', 'prey_5', 'predator_2', 'prey_8', 'prey_11', 'prey_6', 'prey_9', 'predator_1', 'prey_7'}\n",
      "4\n",
      "get_agent_ids {'predator_0', 'prey_4', 'predator_3', 'prey_10', 'prey_5', 'predator_2', 'prey_8', 'prey_11', 'prey_6', 'prey_9', 'predator_1', 'prey_7'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/utils/pre_checks/env.py\", line 307, in check_multiagent_environments\n    obs_and_infos = env.reset(seed=42, options={})\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/rllib/predpreygrass/environments/predpreygrass_env.py\", line 294, in reset\n    self.agent_observation_dict[agent_name] = self.observe(agent_name)\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/rllib/predpreygrass/environments/predpreygrass_env.py\", line 535, in observe\n    observation[i][0:max,max-1-j] = 0\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\nIndexError: index 6 is out of bounds for axis 1 with size 4\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\n    check_multiagent_environments(env)\n  File \"/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/utils/pre_checks/env.py\", line 312, in check_multiagent_environments\n    raise ValueError(\nValueError: Your environment (<PredPreyGrassEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `reset()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/utils/pre_checks/env.py:307\u001b[0m, in \u001b[0;36mcheck_multiagent_environments\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     obs_and_infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# No more gym < 0.26 support! Error and explain the user how to upgrade to\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# gymnasium.\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/rllib/predpreygrass/environments/predpreygrass_env.py:294\u001b[0m, in \u001b[0;36mPredPreyGrassEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_name_list:\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_observation_dict[agent_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_cycles \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/rllib/predpreygrass/environments/predpreygrass_env.py:535\u001b[0m, in \u001b[0;36mPredPreyGrassEnv.observe\u001b[0;34m(self, agent_name)\u001b[0m\n\u001b[1;32m    534\u001b[0m observation[i][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mmax\u001b[39m,j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 535\u001b[0m \u001b[43mobservation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    536\u001b[0m observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mswapaxes(observation, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 4",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/utils/pre_checks/env.py:81\u001b[0m, in \u001b[0;36mcheck_env\u001b[0;34m(env, config)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, MultiAgentEnv):\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mcheck_multiagent_environments\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, VectorEnv):\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/utils/pre_checks/env.py:312\u001b[0m, in \u001b[0;36mcheck_multiagent_environments\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         ERR_MSG_OLD_GYM_API\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    314\u001b[0m             env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn particular, the `reset()` method seems to be faulty.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    317\u001b[0m reset_obs, reset_infos \u001b[38;5;241m=\u001b[39m obs_and_infos\n",
      "\u001b[0;31mValueError\u001b[0m: Your environment (<PredPreyGrassEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `reset()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpre_checks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  check_env\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcheck_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPredPreyGrassEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/utils/pre_checks/env.py:96\u001b[0m, in \u001b[0;36mcheck_env\u001b[0;34m(env, config)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     actual_error \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe above error has been found in your environment! \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve added a module for checking your custom environments. It \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmay cause your experiment to fail if your environment is not set up \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectly. You can disable this behavior via calling `config.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment(disable_env_checking=True)`. You can run the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment checking module standalone by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mray.rllib.utils.check_env([your env]).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/utils/pre_checks/env.py\", line 307, in check_multiagent_environments\n    obs_and_infos = env.reset(seed=42, options={})\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/rllib/predpreygrass/environments/predpreygrass_env.py\", line 294, in reset\n    self.agent_observation_dict[agent_name] = self.observe(agent_name)\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/rllib/predpreygrass/environments/predpreygrass_env.py\", line 535, in observe\n    observation[i][0:max,max-1-j] = 0\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\nIndexError: index 6 is out of bounds for axis 1 with size 4\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\n    check_multiagent_environments(env)\n  File \"/home/doesburg/Dropbox/03_marl_code/PredPreyGrass/.conda/lib/python3.11/site-packages/ray/rllib/utils/pre_checks/env.py\", line 312, in check_multiagent_environments\n    raise ValueError(\nValueError: Your environment (<PredPreyGrassEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `reset()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env])."
     ]
    }
   ],
   "source": [
    "from environments.predpreygrass_env import PredPreyGrassEnv\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.utils.pre_checks.env import  check_env\n",
    "import time\n",
    "check_env(PredPreyGrassEnv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cf48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(PredPreyGrassEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
    "    .environment(env=PredPreyGrassEnv,disable_env_checking=False)\n",
    "    .multi_agent(\n",
    "        policies=[\"policy1\", \"policy2\"],\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "    )\n",
    ")\n",
    "algo = config.build()\n",
    "\n",
    "env = PredPreyGrassEnv(config={\"render\": True})\n",
    "obs, _ = env.reset()\n",
    "terminateds = {\"__all__\" : False}\n",
    "truncateds = {\"__all__\" : False}\n",
    "    \n",
    "while not terminateds[\"__all__\"] or not truncateds[\"__all__\"]:\n",
    "\n",
    "    action1 = algo.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = algo.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, dones, _, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "algo.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#registering the environment\n",
    "from environments.predpreygrass_env import PredPreyGrassEnv\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return PredPreyGrassEnv(env_config=env_config)  # return an env instance\n",
    "\n",
    "register_env(\"pred_prey_grass\", env_creator)\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
    "    .environment(env=\"pred_prey_grass\", disable_env_checking=False)\n",
    "    .multi_agent(\n",
    "        policies=[\"policy1\", \"policy2\"],\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "env = PredPreyGrassEnv(env_config={\"render\": True})\n",
    "obs, _ = env.reset()\n",
    "dones = {\"__all__\" : False}\n",
    "    \n",
    "while not dones[\"__all__\"]:\n",
    "\n",
    "    action1 = algo.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = algo.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, dones, _, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "algo.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a0427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define policies in dict with PolicySpec\n",
    "from environments.predpreygrass_env import PredPreyGrassEnv\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import time\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return PredPreyGrassEnv(env_config=env_config)  # return an env instance\n",
    "\n",
    "register_env(\"pred_prey_grass\", env_creator)\n",
    "\n",
    "policies = { \n",
    "    \"policy1\": PolicySpec(None,observation_space=PredPreyGrassEnv.observation_space, action_space=PredPreyGrassEnv.action_space),\n",
    "    \"policy2\": PolicySpec(None,observation_space=PredPreyGrassEnv.observation_space, action_space=PredPreyGrassEnv.action_space)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"pred_prey_grass\", disable_env_checking=False)\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id.starts_with == \"agent1\" else \"policy2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\"\"\"\n",
    "config_env = {\"render\": True}\n",
    "\n",
    "\n",
    "env = PredPreyGrassEnv(config=config_env)\n",
    "obs, _ = env.reset()\n",
    "dones = {\"__all__\" : False}\n",
    "\"\"\"\n",
    "    \n",
    "\"\"\"\n",
    "while not dones[\"__all__\"]:\n",
    "\n",
    "    action1 = algo.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = algo.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, dones, _, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "algo.stop()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define policies in dict with PolicySpec\n",
    "#add width and height parameters to the environment\n",
    "\n",
    "\n",
    "from multi_agent_env_ray_2_9_3 import MultiAgentArena\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import time\n",
    "\n",
    "MAX_WIDTH = 100\n",
    "MAX_HEIGHT = 100\n",
    "                                                   \n",
    "def env_creator(env_config):\n",
    "    print(env_config)\n",
    "    return MultiAgentArena(config=env_config, width=MAX_WIDTH, height=MAX_HEIGHT)  # return an env instance\n",
    "register_env(\"multi_agent_arena\", env_creator)\n",
    "\n",
    "policies = { \"policy1\": PolicySpec(), \"policy2\": PolicySpec() }\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    if agent_id == \"agent1\":\n",
    "        return \"policy1\"\n",
    "    else:\n",
    "        return \"policy2\"\n",
    "\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"multi_agent_arena\")\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "config_env = {\"render\": True}\n",
    "\n",
    "\n",
    "env = MultiAgentArena(config={\"render\": True}, width=10, height=10)\n",
    "obs, _ = env.reset()\n",
    "dones = {\"__all__\" : False}\n",
    "\n",
    "    \n",
    "\n",
    "while not dones[\"__all__\"]:\n",
    "\n",
    "    action1 = algo.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = algo.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, dones, _, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "algo.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define policies in dict with PolicySpec\n",
    "#add width and height parameters to the environment\n",
    "#train the environment with the algo\n",
    "\n",
    "from multi_agent_env_ray_2_9_3 import MultiAgentArena\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import time\n",
    "\n",
    "MAX_WIDTH = 10\n",
    "MAX_HEIGHT = 10\n",
    "                                                   \n",
    "def env_creator(env_config):\n",
    "    print(env_config)\n",
    "    return MultiAgentArena(config=env_config, width=MAX_WIDTH, height=MAX_HEIGHT)  # return an env instance\n",
    "register_env(\"multi_agent_arena\", env_creator)\n",
    "\n",
    "policies = { \"policy1\": PolicySpec(), \"policy2\": PolicySpec() }\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    if agent_id == \"agent1\":\n",
    "        return \"policy1\"\n",
    "    else:\n",
    "        return \"policy2\"\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"multi_agent_arena\")\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "\n",
    "\n",
    "config_env = {\"render\": True}\n",
    "\n",
    "\n",
    "env = MultiAgentArena(config={\"render\": True}, width=10, height=10)\n",
    "obs, _ = env.reset()\n",
    "dones = {\"__all__\" : False}\n",
    "    \n",
    "\n",
    "while not dones[\"__all__\"]:\n",
    "\n",
    "    action1 = algo.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = algo.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, dones, _, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "algo.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e06d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define policies in dict with PolicySpec\n",
    "#add width and height parameters to the environment\n",
    "#train the environment with the algo\n",
    "#use tune for a gridsearch\n",
    "\n",
    "import ray\n",
    "from ray import train, tune\n",
    "\n",
    "from multi_agent_env_ray_2_9_3 import MultiAgentArena\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import time\n",
    "\n",
    "MAX_WIDTH = 10\n",
    "MAX_HEIGHT = 10\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "                                                   \n",
    "def env_creator(env_config):\n",
    "    print(env_config)\n",
    "    return MultiAgentArena(config=env_config, width=MAX_WIDTH, height=MAX_HEIGHT)  # return an env instance\n",
    "register_env(\"multi_agent_arena\", env_creator)\n",
    "\n",
    "policies = { \"policy1\": PolicySpec(), \"policy2\": PolicySpec() }\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    if agent_id == \"agent1\":\n",
    "        return \"policy1\"\n",
    "    else:\n",
    "        return \"policy2\"\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"multi_agent_arena\")\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]},lr=tune.grid_search([0.01, 0.001, 0.0001]))\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"episode_reward_mean\": 15},\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "tuner.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define policies in dict with PolicySpec\n",
    "#add width and height parameters to the environment\n",
    "#train the environment with the algo\n",
    "#use tune for a gridsearch\n",
    "#retrieving the checkpoint(s) of the trained agent\n",
    "\n",
    "import ray\n",
    "from ray import train, tune\n",
    "\n",
    "from multi_agent_env_ray_2_9_3 import MultiAgentArena\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import time\n",
    "\n",
    "MAX_WIDTH = 10\n",
    "MAX_HEIGHT = 10\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "                                                   \n",
    "def env_creator(env_config):\n",
    "    print(env_config)\n",
    "    return MultiAgentArena(config=env_config, width=MAX_WIDTH, height=MAX_HEIGHT)  # return an env instance\n",
    "register_env(\"multi_agent_arena\", env_creator)\n",
    "\n",
    "policies = { \"policy1\": PolicySpec(), \"policy2\": PolicySpec() }\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    if agent_id == \"agent1\":\n",
    "        return \"policy1\"\n",
    "    else:\n",
    "        return \"policy2\"\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"multi_agent_arena\")\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0,log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]},lr=tune.grid_search([0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]))\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# ``Tuner.fit()`` allows setting a custom log directory (other than ``~/ray-results``)\n",
    "tuner = ray.tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"episode_reward_mean\": 10},\n",
    "        checkpoint_config=train.CheckpointConfig(checkpoint_at_end=True),\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "# Get the best result based on a particular metric.\n",
    "best_result = results.get_best_result(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# Get the best checkpoint corresponding to the best result.\n",
    "best_checkpoint = best_result.checkpoint\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
