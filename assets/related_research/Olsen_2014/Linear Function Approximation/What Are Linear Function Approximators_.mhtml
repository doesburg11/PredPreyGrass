From: <Saved by Blink>
Snapshot-Content-Location: https://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43/lib/photoz/.g/web/glossary/linear.html
Subject: What Are Linear Function Approximators?
Date: Sun, 19 Jun 2022 10:25:04 -0000
MIME-Version: 1.0
Content-Type: multipart/related;
	type="text/html";
	boundary="----MultipartBoundary--70WBDA3TqLNkfHHWM41EmVNiMhkmcBsnslUYkkLbkM----"


------MultipartBoundary--70WBDA3TqLNkfHHWM41EmVNiMhkmcBsnslUYkkLbkM----
Content-Type: text/html
Content-ID: <frame-83C812462AAB2B87E5B5FD025C42DAC3@mhtml.blink>
Content-Transfer-Encoding: quoted-printable
Content-Location: https://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43/lib/photoz/.g/web/glossary/linear.html

<html><head><meta http-equiv=3D"Content-Type" content=3D"text/html; charset=
=3Dwindows-1252">
<title>What Are Linear Function Approximators?</title>

<meta name=3D"author" content=3D"Leemon Baird leemon@cs.cmu.edu">
<meta name=3D"date_created" content=3D"27 May 1998">
<meta name=3D"date_modified" content=3D"27 May 1998">

<meta name=3D"description" content=3D"This is the 'linear function approxim=
ator' entry in the=20
machine learning glossary at Carnegie Mellon University. =20
Each entry includes a short definition for the term along=20
with a bibliography and links to related Web pages.">

<meta name=3D"keywords" content=3D"linear, function, approximator,
approximation, backprop, neural network, reinforcement learning,=20
machine learning, AUTON">

</head><body bgcolor=3D"#ffffff">
<center><h1>Linear Function Approximation <img src=3D"https://www.cs.cmu.ed=
u/afs/cs.cmu.edu/project/learn-43/lib/photoz/.g/web/glossary/logo.gif" alt=
=3D"" align=3D"middle"></h1></center>

A linear function approximator is a function <em>y=3Df(x,w)</em> that is li=
near
in the weights, though not necessarily linear in the input <em>x</em>:<pre>=
    y =3D w_1 * f_1(x) + w_2 * f_2(x) + ... + w_n * f_n(x)
</pre>
where <em>x</em>, <em>y</em>, and <em>w</em> can be vectors,=20
the <em>f_i</em>() functions can be linear=20
or nonlinear, and <em>w_i</em> is the <em>i</em>th=20
element of the <em>w</em> vector.  Examples of linear function approximator=
s
include:
<ul>
<li><b>Lookup table</b><br>
    There is a separate weight for each possible value of <em>x</em>.
    There are only <em>n</em> possible values for <em>x</em>,=20
    and <em>f_i(x)=3D1</em>
    when <em>x=3Di</em> and <em>f_i(x)</em>=3D0 otherwise.
</li><li><b>Linear</b><br>
    The output is just the dot product of <em>w</em> and <em>x</em>.
    The individual functions are just <em>f_i(x)=3Dx_i</em>, where=20
    <em>x_i</em> is the <em>i</em>th
    element of vector <em>x</em>. =20
</li><li><b>Radial Basis Functions</b><br>
    Each <em>f_i(x)</em> function looks like a smooth bump.
    Each <em>f_i()</em> function has a "center" location, and=20
    <em>f_i(x)</em> is a monotonic function of the distance=20
    from <em>x</em> to the center.  The "distance" may be
    Euclidean distance (circular bumps), or there may be=20
    a diagonal covariance matrix (ellipsoidal bumps parallel to
    the axes), or there may be a full covariance matrix (general
    ellipsoidal bumps).  To be a linear function approximator,
    the bumps must not move or change shape.
</li><li><b>Wavelets</b><br>
    Each f_i(x) is a wavelet, typically the product of a=20
    cosine and a Gaussian.  This is particularly useful in
    image applications, because the human visual system seems
    to use similar functions.
</li><li><b>CMAC</b><br>
    Each <em>f_i(x)</em> function has a value of 1 inside of=20
    <em>k</em> square regions
    in input space, and 0 everywhere else.  A hash function is used
    to make sure that the <em>k</em> squares are randomly scattered.  The=
=20
    functions are chosen so that for any give <em>x</em>, there will be=20
    exactly <em>c</em> different <em>f_i(x)</em> functions active, and=20
    their square regions will be offset from one another slightly.
    Because of the hash function, the CMAC has the nice property that
    it makes good use of its weights even when all the training=20
    examples are in one small region of input space which wasn't known
    beforehand.  One particularly interesting example of this is when
    the input space is very high dimensional, but all training=20
    examples come from a simple, low-dimensional manifold.  For some
    reason, people often implement CMACs with <em>k</em>=3D1, which=20
    destroys this useful property.  The hash function was originally
    proposed to be reminiscent of random neuron wiring in the brain.
</li></ul>
Linear function approximators have several nice properties.  For=20
supervised learning, the weights can be found immediately
at the cost of a single matrix inversion, without any=20
gradient-descent or incremental learning.  For=20
<a href=3D"https://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43/lib/photo=
z/.g/web/glossary/rl.html">reinforcement learning</a>,=20
the weights can be found at the cost of solving a single
<a href=3D"https://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43/lib/photo=
z/.g/web/glossary/lp.html">linear program</a>.
<p>
For incremental reinforcement learning algorithms there are also
a few useful properties.  For a lookup table, almost all the
incremental algorithms are guaranteed to converge to optimality.
For other linear function approximators, TD(lambda) is guaranteed
to converge when doing on-policy training (transitions are trained
on with a frequency proportional to their frequency in the Markov
chain).
</p><p>
Unfortunately, very few other convergence results are true.  TD(lambda)
can diverge for off-policy training.  Q-learning can diverge even
with on-policy training.  SARSA can oscillate wildly and periodically
forget everything useful it had learned so far.  For incremental=20
algorithms, limiting the function approximator to linear function
approximators does not help convergence very much.
</p><p>
Fortunately there are ways to ensure that all these algorithms will
converge: use the the <a href=3D"https://www.cs.cmu.edu/afs/cs.cmu.edu/proj=
ect/learn-43/lib/photoz/.g/web/glossary/residual.html">residual</a> form of
each algorithm.  In that case, they will converge for both linear
and nonlinear function approximators.
</p><p>
There are also results indicating that nonlinear function approximators
may be more powerful in general than linear function approximators for
learning high-dimensional functions.  For example, if the target
function is fairly smooth (has little energy in the high requencies),
and the function approximator is nonlinear,
then it is known that the number of weights needed for a good fit=20
grows only polynomially with the dimensionality of the input.=20
This is true for such diverse function approximators as=20
sigmoidal <a href=3D"https://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43=
/lib/photoz/.g/web/glossary/neural.html">neural networks</a> and linear
combinations of sine waves.  It may even be true for all of the=20
popular nonlinear function approximators.  But it has been shown
that it is not true for any linear function approximator.  This=20
result suggests that linear function approximators may be most=20
useful in cases where the input space is low dimensional, or where
the training examples all come from a low-dimensional manifold
in a high-dimensional space.


</p><h2>More Information</h2>
<ul>
</ul>

<i>Back to <a href=3D"https://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-4=
3/lib/photoz/.g/web/glossary/index.html">Glossary Index</a></i>


</body></html>
------MultipartBoundary--70WBDA3TqLNkfHHWM41EmVNiMhkmcBsnslUYkkLbkM----
Content-Type: image/gif
Content-Transfer-Encoding: base64
Content-Location: https://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43/lib/photoz/.g/web/glossary/logo.gif

R0lGODlhAQABAIAAAP8A//8A/yH+Dk1hZGUgd2l0aCBHSU1QACH5BAEKAAAALAAAAAABAAEAAAIC
RAEAOw==

------MultipartBoundary--70WBDA3TqLNkfHHWM41EmVNiMhkmcBsnslUYkkLbkM------
